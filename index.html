<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL</title>

    <!-- 添加favicon -->
    <!-- <link rel="icon" type="image/x-icon" href="assets/images/icon.png"> -->
    <!-- 或者使用PNG格式 -->
    <link rel="icon" type="image/png" href="assets/images/icon.png">

    <script>
        var task_map = {
            "simple-object-manipulation": "simple_object_manipulation",
            "visual-goal-reaching": "visual_goal_reaching",
            "novel-concept-grounding": "novel_concept_grounding",
            "one-shot-video-imitation": "one_shot_video_imitation",
            "visual-constraint-satisfaction": "visual_constraint_satisfaction",
            "visual-reasoning": "visual_reasoning"
        };

        function updateDemoVideo(category) {
            // var demo = document.getElementById("single-menu-demos").value;
            var task = document.getElementById(category + "-menu-tasks").value;
            var inst = document.getElementById(category + "-menu-instances").value;

            console.log(task_map[category], task, inst)

            var video = document.getElementById(category + "-single-task-video");
            video.src = "assets/videos/demos/" +
                task_map[category] +
                "/" +
                task +
                "/" +
                inst +
                ".mp4";
            video.playbackRate = 2.0;
            video.play();
        }
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <style>
      .content.has-text-justified p,
      .content.has-text-justified ul,
      .content.has-text-justified ol {
          font-size: 120%;  /* 可以调整这个百分比来改变大小 */
      }

      .content.has-text-justified li {
        font-size: inherit;  /* 继承父元素的字体大小 */
      }
      
      .hover-card p,
      .hover-card ul,
      .hover-card ol {
          font-size: 120%;
      }
      .hover-card li {
        font-size: inherit;
    }
  </style>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>



<d-contents>
  <nav>
      <h4>CONTENTS</h4>
      <div><a href="#Abstract">Abstract</a></div>
      <div><a href="#Method">Method</a></div>
      <div><a href="#Performance">Performance</a></div>
      <div><a href="#Conclusion">Conclusion</a></div>
  </nav>
</d-contents>




<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-2 publication-title">Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL</h1>
                    <!-- <h1 class="title is-2 custom-heading">Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL</h1> -->
                    <div class="is-size-5 publication-authors">
                        <span class="author-block">
                          <a target="_blank" href="https://adatwi.github.io/">Xintong Zhang</a><sup>1,2*</sup>,
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://zhigao2017.github.io/">Zhi Gao</a><sup>2,3*</sup>,
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://bofei5675.github.io/">Bofei Zhang</a><sup>2</sup>,
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://pengxiang-li.github.io/">Pengxiang Li</a><sup>1,2</sup>,
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://adatwi.github.io/">Xiaowen Zhang</a><sup>2</sup>,
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://adatwi.github.io/">Yang Liu</a><sup>2</sup>,
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://adatwi.github.io/">Tao Yuan</a><sup>2</sup>
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://wu-yuwei-bit.github.io/">Yuwei Wu</a><sup>1,4†</sup>
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://scholar.google.com/citations?user=Sl6TV7gAAAAJ&hl=en">Yunde Jia</a><sup>4</sup>
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://www.zhusongchun.net/">Song-Chun Zhu</a><sup>2,3,5</sup>
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://liqing.io/">Qing Li</a><sup>2†</sup>
                        </span>


          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Beijing Key Laboratory of Intelligent Information Technology, School of Computer Science & Technology, Beijing Institute of Technology, <sup>2</sup>State Key Laboratory of General Artificial Intelligence, BIGAI, <sup>3</sup>School of Intelligence Science and Technology, Peking University, <sup>4</sup>Guangdong Laboratory of Machine Perception and Intelligent Computing, Shenzhen MSU-BIT University, <sup>5</sup>Department of Automation, Tsinghua University</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal contribution, <sup>†</sup>Corresponding authors</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2505.15436"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a target="_blank" href="https://github.com/..."
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              
              <img src="assets/images/teaser.jpg" class="interpolation-image"
                alt="" style="display: block; margin-left: auto; margin-right: auto;
                margin-top: 2rem;
                width: 1000px; box-shadow: 0 8px 24px rgba(0, 0, 0, 0.2);" />
              
            </div>
          </div>
        </div>
    </div>
</section>




<section class="hero is-light" id="Abstract">
  <div class="hero-body">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3 custom-heading">Abstract</h2>
        <div class="content has-text-justified">
          <!-- <img src="assets/images/teaser.jpg" class="interpolation-image"
               alt="" style="display: block; margin-left: auto; margin-right: auto" width="900"/> -->
          <p style="font-size: 125%">
            Vision language models (VLMs) have achieved impressive performance across a variety of computer vision tasks. However, the multimodal reasoning capability has not been fully explored in existing models. In this paper, we propose a Chain-of-Focus (CoF) method that allows VLMs to perform adaptive focusing and zooming in on key image regions based on obtained visual cues and the given questions, achieving efficient multimodal reasoning. To enable this CoF capability, we present a two-stage training pipeline, including supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct the MM-CoF dataset, comprising 3K samples derived from a visual agent designed to adaptively identify key regions to solve visual tasks with different image resolutions and questions. We use MM-CoF to fine-tune the Qwen2.5-VL model for cold start. In the RL stage, we leverage the outcome accuracies and formats as rewards to update the Qwen2.5-VL model, enabling further refining the model's search and reasoning strategy without human priors. Our model achieves significant improvements on multiple benchmarks. On the V* benchmark that requires strong visual reasoning capability, our model outperforms existing VLMs by 5% among 8 image different resolutions ranging from 224 to 4K, demonstrating the effectiveness of the proposed CoF method and facilitating the more efficient deployment of VLMs in practical applications.
          </p>
        </div>
      </div>
    </div>
  </div>
</div>
</section>



<section class="section" id="Method">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <!-- <h2 class="title is-3" style="text-shadow: 1px 1px 4px rgba(0, 0, 0, 0.3);">Method</h2> -->

        <h2 class="title is-3 section-title" style="text-shadow: 1px 1px 4px rgba(0, 0, 0, 0.3);">Method</h2>
        <hr class="section-line">

        <div class="content has-text-justified">
          <img src="assets/images/model_inference.jpg" class="interpolation-image"
               alt="" style="display: block; margin-left: auto; margin-right: auto" width="900"/>
               <div class="content has-text-justified">
                <div class="content has-text-justified hover-card" style="padding: 1.5rem;">
                  <p>The <strong>Chain-of-Focus (CoF)</strong> method enables VLMs to perform adaptive search and zooming in on key image regions, thereby creating a chain of focus steps for multimodal reasoning with gradually obtained visual cues.</p>
                  <ol>
                    <li>When the given image is <strong>high-resolution</strong> and uses a large number of visual tokens, or when the question depends on a large region of the image, the extracted visual tokens are often sufficient to answer the question directly. 
                    <li>When the image is <strong>low-resolution</strong> with fewer visual tokens, or when the question demands details from small regions of the image, the visual tokens may not provide enough cues. In this case, the model should search for and zoom in on key image regions to extract more visual cues. 
                  </ol>
                  <p>In implementation, the visual tokens corresponding to key regions are appended to previously generated output tokens for subsequent outputs during a single generation round. This approach allows the VLMs to gather more visual cues, enabling them to analyze the image more thoroughly, accurately, and reliably than if they only relied on a static view of the image. </p>
                    
                  <p><span style="color:#6a7bff; font-weight:bold;">Note </span> that our method does not perform visual search and zooming for every image, but performs adaptive search and zooming based on obtained visual cues, reducing the cost while keeping the performance.</p>
                </div>
          <img src="assets/images/agent.jpg" class="interpolation-image"
            alt="" style="display: block; margin-left: auto; margin-right: auto" width="900"/>
            <div class="content has-text-justified">
              <div class="content has-text-justified hover-card" style="padding: 1.5rem;">
                <p>CoF adopts a two-stage training pipeline.</p>
                <p><span style="color:#6a7bff; font-weight:bold;">In the SFT stage</span>, we construct the MM-CoF dataset with 3K samples from the SAM dataset across diverse resolutions. For each image, we synthesize a task and use a visual agent with multiple tools to search and reason until task completion. The agent's reasoning steps are then summarized into a CoF process by an LLM. We fine-tune a Qwen2.5-VL-7B model on MM-CoF for cold start.</p>
                <p><span style="color:#ff6b6b; font-weight:bold;">In the RL stage</span>, we leverage the outcome accuracies and formats as rewards to update the VLMs, enabling further refining the model’s search and reasoning strategy without human priors. We denote the obtained model as Qwen2.5-VL-7B-CoF.</p>
                </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- <hr style="width: 60%; border-top: 1px solid #e0e0e0; margin: 40px auto;"> -->

<section class="section" id="Performance">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <!-- <h2 class="title is-3" style="text-shadow: 1px 1px 4px rgba(0, 0, 0, 0.3);">Performance</h2> -->

        <h2 class="title is-3 section-title" style="text-shadow: 1px 1px 4px rgba(0, 0, 0, 0.3);">Performance</h2>
        <hr class="section-line">
    

        <!-- <div class="content has-text-justified"> -->


        <!-- case 1 -->
        <div class="columns is-vcentered">
          
          <!-- img -->
          <div class="column is-6">
            <figure class="image">
              <!-- <img src="assets/images/case1.jpg" alt="hhh" style="border-radius: 8px; box-shadow: 0 0 16px rgba(0,0,0,0.1);" /> -->
              <img src="assets/images/case1.jpg" style="border-radius: 8px; box-shadow: 0 0 16px rgba(0,0,0,0.1);" />

            </figure>
          </div>


          <!-- text -->
          <div class="column is-6 has-text-left reasoning-block">
            <p>
              <strong>Query:</strong> Is the color of the bus red or yellow?<br>
              (A) yellow<br>
              (B) red<br>
              Answer with the option's letter from the given choices directly.
            </p>
          
            <p><strong>Output:</strong></p>
          
            <p class="justified-output">
              <span class="tag-label">&lt;think&gt;</span>
              To determine the color of the bus, I need to closely observe the bus in the image.
              The bus is clearly visible, and its color can be seen distinctly as red.
              <span class="tag-label">&lt;/think&gt;</span>
            </p>
          
            <p><span class="tag-label">&lt;answer&gt;</span> B <span class="tag-label">&lt;/answer&gt;</span></p>
          </div>

        </div>






        <!-- case 2 -->
        <div class="columns is-vcentered">
          
          <!-- img -->
          <div class="column is-6">
            <figure class="image">
              <!-- <img src="assets/images/case1.jpg" alt="hhh" style="border-radius: 8px; box-shadow: 0 0 16px rgba(0,0,0,0.1);" /> -->
              <img src="assets/images/case2.jpg" style="border-radius: 8px; box-shadow: 0 0 16px rgba(0,0,0,0.1);" />

            </figure>
          </div>


          <!-- text -->
          <div class="column is-6 has-text-left reasoning-block">
            <p>
              <strong>Query:</strong> What is the color of the woman's jacket?<br>
              (A) black<br>
              (B) red<br>
              (C) blue<br>
              (D) orange<br>
              Answer with the option's letter from the given choices directly.
            </p>
          
            <p><strong>Output:</strong></p>
          
            <p class="justified-output">
              <span class="tag-label">&lt;think&gt;</span>
              To determine the color of the woman's jacket, I need to closely examine the area where she is located. However, the color details are not immediately clear due to the distance and lighting conditions. To improve visibility, I proceed step by step. I first identify the bounding box corresponding to the woman: 
              <span class="bbox-label">&lt;|box_start|&gt;</span>[570, 1726, 615, 1848]<span class="bbox-label">&lt;|box_end|&gt;</span>. 
              <!-- <span class="bbox-label">&lt;|box_start|&gt; [570, 1726, 615, 1848] &lt;|box_end|&gt;</span>.  -->
              I zoom in on this area to obtain a refined visual embedding 
              <span class="zoomin-label">&lt;|image_zoomin|&gt;</span>, 
              which reveals that the jacket is light blue. 
              <span class="tag-label">&lt;/think&gt;</span>
            </p>
          
            <p><span class="tag-label">&lt;answer&gt;</span> C <span class="tag-label">&lt;/answer&gt;</span></p>
          </div>
            
        </div>









        <!-- case 3 -->
        <div class="columns is-vcentered">
          
          <!-- img -->
          <div class="column is-6">
            <figure class="image">
              <!-- <img src="assets/images/case1.jpg" alt="hhh" style="border-radius: 8px; box-shadow: 0 0 16px rgba(0,0,0,0.1);" /> -->
              <img src="assets/images/case3.jpg" style="border-radius: 8px; box-shadow: 0 0 16px rgba(0,0,0,0.1);" />

            </figure>
          </div>


          <!-- text -->
          <div class="column is-6 has-text-left reasoning-block">
            <p>
              <strong>Query:</strong> What is the color of the lock?<br>
              (A) yellow<br>
              (B) silver<br>
              (C) golden<br>
              (D) red<br>
              Answer with the option's letter from the given choices directly.
            </p>
          
            <p><strong>Output:</strong></p>
          
            <p class="justified-output">
              <span class="tag-label">&lt;think&gt;</span>
              To determine the color of the lock, I need to closely examine the area where the lock is located. However, the lock is not immediately visible, making it difficult to identify its color. To improve visibility, I proceed step by step. I first attempt to locate the lock but find no clear indication due to its small size. I then focus on the red postbox, identifying two bounding boxes:
              <span class="bbox-label">&lt;|box_start|&gt;</span>[585, 700, 1305, 1821]<span class="bbox-label">&lt;|box_end|&gt;</span> and 
              <!-- <span class="bbox-label">&lt;|box_start|&gt;</span>[585, 700, 1305, 1821]<span class="bbox-label">&lt;|box_end|&gt;</span> and  -->
              <span class="bbox-label">&lt;|box_start|&gt;</span>[645, 920, 1275, 1821]<span class="bbox-label">&lt;|box_end|&gt;</span>. I analyze these areas but still cannot locate the lock. Next, I shift my attention to the area above the postbox, locating two bounding boxes:
              <!-- <span class="bbox-label">&lt;|box_start|&gt; [645, 920, 1275, 1821] &lt;|box_end|&gt;</span>. I analyze these areas but still cannot locate the lock. Next, I shift my attention to the area above the postbox, locating two bounding boxes: -->
              <span class="bbox-label">&lt;|box_start|&gt;</span>[0, 600, 1425, 1241]<span class="bbox-label">&lt;|box_end|&gt;</span> and 
              <!-- <span class="bbox-label">&lt;|box_start|&gt; [0, 600, 1425, 1241] &lt;|box_end|&gt;</span> and  -->
              <span class="bbox-label">&lt;|box_start|&gt;</span>[0, 600, 1425, 1301]<span class="bbox-label">&lt;|box_end|&gt;</span>. I zoom in on 
              <!-- <span class="bbox-label">&lt;|box_start|&gt; [0, 600, 1425, 1301] &lt;|box_end|&gt;</span>. I zoom in on  -->
              <span class="bbox-label">&lt;|box_start|&gt;</span>[0, 600, 1425, 1241]<span class="bbox-label">&lt;|box_end|&gt;</span> to obtain a refined visual embedding 
              <!-- <span class="bbox-label">&lt;|box_start|&gt; [0, 600, 1425, 1241] &lt;|box_end|&gt;</span> to obtain a refined visual embedding  -->
              <span class="zoomin-label">&lt;|image_zoomin|&gt;</span>, which reveals the presence of a mailbox. Upon analyzing this refined area, I confirm that the lock on the mailbox is silver.
              <span class="tag-label">&lt;/think&gt;</span>
            </p>
          
            <p><span class="tag-label">&lt;answer&gt;</span> B <span class="tag-label">&lt;/answer&gt;</span></p>
          </div>
        </div>





      </div>
    </div>
  </div>
</section>




<section class="hero is-light" id="Conclusion">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <!-- Generalization Videos -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3 custom-heading">Conclusion</h2>
      <div class="column">
          <div class="content has-text-justified">
          <p style="font-size: 125%">
            In this paper, we have presented the chain-of-focus (CoF) method to improve the multimodal reasoning ability of VLMs via adaptive search and focus. By operations of adaptive image zooming, the CoF method can help the reasoning process using more visual cues. The proposed data collection pipeline can efficiently collect CoF data and the data can indeed empower the perception, grounding, and reasoning ability for visual language models. The used SFT-RL training pipeline can gradually improve the generalization of VLMs.        </p>

        </div>
      </div>

    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{zhang2025chainoffocusadaptivevisualsearch,
      title={Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL}, 
      author={Xintong Zhang and Zhi Gao and Bofei Zhang and Pengxiang Li and Xiaowen Zhang and Yang Liu and Tao Yuan and Yuwei Wu and Yunde Jia and Song-Chun Zhu and Qing Li},
      year={2025},
      eprint={2505.15436},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2505.15436}, 
}</code></pre>
  </div>
</section>


<!-- 
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p> Website borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> under a <a
              href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0
              International</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer> -->

<script src="static/js/contents_bar.js"></script>


</body>
</html>
