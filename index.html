<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning</title>

    <!-- 添加favicon -->
    <!-- <link rel="icon" type="image/x-icon" href="assets/images/icon.png"> -->
    <!-- 或者使用PNG格式 -->
    <link rel="icon" type="image/png" href="assets/images/icon.png">

    <script>
        var task_map = {
            "simple-object-manipulation": "simple_object_manipulation",
            "visual-goal-reaching": "visual_goal_reaching",
            "novel-concept-grounding": "novel_concept_grounding",
            "one-shot-video-imitation": "one_shot_video_imitation",
            "visual-constraint-satisfaction": "visual_constraint_satisfaction",
            "visual-reasoning": "visual_reasoning"
        };

        function updateDemoVideo(category) {
            // var demo = document.getElementById("single-menu-demos").value;
            var task = document.getElementById(category + "-menu-tasks").value;
            var inst = document.getElementById(category + "-menu-instances").value;

            console.log(task_map[category], task, inst)

            var video = document.getElementById(category + "-single-task-video");
            video.src = "assets/videos/demos/" +
                task_map[category] +
                "/" +
                task +
                "/" +
                inst +
                ".mp4";
            video.playbackRate = 2.0;
            video.play();
        }
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <style>
      .content.has-text-justified p,
      .content.has-text-justified ul,
      .content.has-text-justified ol {
          font-size: 120%;  /* 可以调整这个百分比来改变大小 */
      }

      .content.has-text-justified li {
        font-size: inherit;  /* 继承父元素的字体大小 */
      }
      
      .hover-card p,
      .hover-card ul,
      .hover-card ol {
          font-size: 120%;
      }
      .hover-card li {
        font-size: inherit;
    }
  </style>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>



<d-contents>
  <nav>
      <h4>CONTENTS</h4>
      <div><a href="#Abstract">Abstract</a></div>
      <div><a href="#Method">Method</a></div>
      <div><a href="#Performance">Performance</a></div>
      <div><a href="#Visualization">Visualization</a></div>
  </nav>
</d-contents>




<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-2 publication-title">STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning</h1>
                    <!-- <h1 class="title is-2 custom-heading">STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning</h1> -->
                    <div class="is-size-5 publication-authors">
                        <span class="author-block">
                          <a target="_blank" href="https://xiaowzhang00.github.io/">Xiaowen Zhang</a><sup>1,2</sup>,
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://zhigao2017.github.io/">Zhi Gao</a><sup>2,3*</sup>,
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://xiaowzhang00.github.io/">Licheng Jiao</a><sup>1†</sup>,
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://xiaowzhang00.github.io/">Lingling Li</a><sup>1</sup>,
                        </span>
                        <span class="author-block">
                          <a target="_blank" href="https://liqing.io/">Qing Li</a><sup>2†</sup>
                        </span>

          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Xidian University, <sup>2</sup>State Key Laboratory of General Artificial Intelligence, BIGAI, <sup>3</sup>Beijing Institute of Technology, </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal contribution, <sup>†</sup>Corresponding authors</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2505.15436"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a target="_blank" href="https://github.com/xtong-zhang/Chain-of-Focus"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a target="_blank" href="https://huggingface.co/xintongzhang/CoF-sft-model-7b"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-robot"></i>
                  </span>
                  <span>Model</span>
                </a>
              </span>
              
              <span class="link-block">
                <a target="_blank" href="https://huggingface.co/datasets/xintongzhang/CoF-SFT-Data-5.4k"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>

              
              <img src="assets/fig1.jpg" class="interpolation-image"
                alt="" style="display: block; margin-left: auto; margin-right: auto;
                margin-top: 2rem;
                width: 1000px; box-shadow: 0 8px 24px rgba(0, 0, 0, 0.2);" />
              
            </div>
          </div>
        </div>
    </div>
</section>




<section class="hero is-light" id="Abstract">
  <div class="hero-body">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <h2 class="title is-3 custom-heading">Abstract</h2>
        <div class="content has-text-justified">
          <!-- <img src="assets/fig1.jpg" class="interpolation-image"
               alt="" style="display: block; margin-left: auto; margin-right: auto" width="900"/> -->
          <p style="font-size: 125%">
            In vision–language models (VLMs), misalignment between textual descriptions and visual coordinates often induces hallucinations. This issue becomes particularly severe in dense prediction tasks such as spatial–temporal video grounding (STVG). Prior approaches typically focus on enhancing visual–textual alignment or attaching auxiliary decoders. However, these strategies inevitably introduce additional trainable modules, leading to significant annotation costs and computational overhead. In this work, we propose a novel visual prompting paradigm that avoids the difficult problem of aligning coordinates across modalities. Specifically, we reformulate per-frame coordinate prediction as a compact instance-level identification problem by assigning each object a unique, temporally consistent ID. These IDs are embedded into the video as visual prompts, providing explicit and interpretable inputs to the VLMs. Furthermore, we introduce STVG-R1, the first reinforcement learning framework for STVG, which employs a task-driven reward to jointly optimize temporal accuracy, spatial consistency, and structural format regularization. Extensive experiments on six benchmarks demonstrate the effectiveness of our approach. STVG-R1 surpasses the baseline Qwen2.5-VL-7B by a remarkable margin of 20.9% on m_IoU on the HCSTVG-v2 benchmark, establishing a new state of the art (SOTA). Surprisingly, STVG-R1 also exhibits strong zero-shot generalization to multi-object referring video object segmentation task, achieving a SOTA 47.3% J&F on MeViS. 
          </p>
        </div>
      </div>
    </div>
  </div>
</div>
</section>



<section class="section" id="Method">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <!-- <h2 class="title is-3" style="text-shadow: 1px 1px 4px rgba(0, 0, 0, 0.3);">Method</h2> -->

        <h2 class="title is-3 section-title" style="text-shadow: 1px 1px 4px rgba(0, 0, 0, 0.3);">Method</h2>
        <hr class="section-line">

        <div class="content has-text-justified">
          <img src="assets/method.jpg" class="interpolation-image"
               alt="" style="display: block; margin-left: auto; margin-right: auto" width="900"/>
               <div class="content has-text-justified">
                <div class="content has-text-justified hover-card" style="padding: 1.5rem;">
                  An illustration of our proposed <strong>STVG-R1</strong> framework. Each object is assigned a unique ID via visual prompts, and the policy model is trained with spatial, temporal, and template rewards.</p>
                  <p><strong>1. Object-Centric Visual Prompting.</strong> Each object in the video is assigned a unique, temporally consistent ID rendered as a small visual marker. These IDs transform dense coordinate prediction into a compact instance-level identification task, mitigating visual–textual misalignment and enabling interpretable grounding. 
                  <p><strong>2. Reinforcement Learning with Structured Rewards.</strong> The model is optimized under a task-driven reward combining temporal accuracy, spatial consistency, and format regularity. This unified framework enhances reasoning coherence and yields state-of-the-art performance across six video grounding benchmarks. 
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <hr style="width: 60%; border-top: 1px solid #e0e0e0; margin: 40px auto;"> -->
        

<section class="section" id="Performance">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <!-- <h2 class="title is-3" style="text-shadow: 1px 1px 4px rgba(0, 0, 0, 0.3);">Performance</h2> -->

        <h2 class="title is-3 section-title" style="text-shadow: 1px 1px 4px rgba(0, 0, 0, 0.3);">Performance</h2>
        <hr class="section-line">

        <div class="content has-text-justified">
            <p>We evaluate STVG-R1 on <strong>six</strong> benchmarks, covering <strong>spatial–temporal video grounding (STVG)</strong>, <strong>spatial video grounding (SVG)</strong>, <strong>temporal video grounding (VTG)</strong>, and <strong>referring video object segmentation (RVOS)</strong>. 
            <p>The results show that our method consistently surpasses both task-specific dense prediction models and general-purpose vision–language models (VLMs), validating the effectiveness of our object-centric visual prompting paradigm and reinforcement learning framework.
            
            <p class="mb-1">
              <strong>1. STVG Results.</strong>
              STVG-R1 achieves new state-of-the-art (SOTA) results on HCSTVG-v1, HCSTVG-v2, and ST-Align,
              improving the baseline Qwen2.5-VL-7B by up to +20.9% m_vIoU on HCSTVG-v2. Compared with the strongest
              SFT-based model SpaceVLLM, our method achieves gains of +4.0%, +6.2%, +10.9%, and +14.1% across four metrics.
            </p>
            
            <figure class="image mt-3">
              <img src="assets/table1.jpg" alt="HCSTVG performance table" style="margin: 0 auto; max-width: 700px; width: 100%; height: auto;">
              <!-- <figcaption class="has-text-grey is-size-7 has-text-centered">HCSTVG-v1/v2 results.</figcaption> -->
            </figure>

            <p class="mb-1">
              <strong>2. SVG and RVOS Results.</strong>
              On ST-Align’s spatial evaluation, STVG-R1 attains m_vIoU 48.6%, exceeding LLaVA-ST by +13.1%. In the more challenging MeViS benchmark, STVG-R1 reaches a J&F of 47.3%, showing strong zero-shot generalization to multi-object segmentation tasks, despite being trained only on single-object grounding data.
            </p>
            
            <figure class="image mt-3">
              <img src="assets/table23.jpg" alt="HCSTVG performance table" style="margin: 0 auto; max-width: 700px; width: 100%; height: auto;">
              <!-- <figcaption class="has-text-grey is-size-7 has-text-centered">HCSTVG-v1/v2 results.</figcaption> -->
            </figure>

            <p class="mb-1">
              <strong>3. VTG Results.</strong>
              For out-of-distribution temporal grounding, STVG-R1 achieves tIoU@0.5 = 52.5% on Charades-STA (zero-shot) and competitive tIoU@0.3 = 42.5% on TVGBench, demonstrating robust temporal reasoning ability.
            </p>
            
            <figure class="image mt-3">
              <img src="assets/table4.jpg" alt="HCSTVG performance table" style="margin: 0 auto; max-width: 700px; width: 100%; height: auto;">
              <!-- <figcaption class="has-text-grey is-size-7 has-text-centered">HCSTVG-v1/v2 results.</figcaption> -->
            </figure>
            
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <hr style="width: 60%; border-top: 1px solid #e0e0e0; margin: 40px auto;"> -->

<section class="section" id="Visualization">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column">
        <!-- <h2 class="title is-3" style="text-shadow: 1px 1px 4px rgba(0, 0, 0, 0.3);">Visualization</h2> -->

        <h2 class="title is-3 section-title" style="text-shadow: 1px 1px 4px rgba(0, 0, 0, 0.3);">Visualization</h2>
        <hr class="section-line">
    

        <!-- <div class="content has-text-justified"> -->


        <!-- case 1 -->
        <div class="columns is-vcentered">
        
          <!-- video -->
          <div class="column is-6">
            <figure class="image">

                <video controls playsinline preload="metadata" style="width:100%;height:auto;">
                  <source src="assets/stvg_demo_v4.mp4?v=6" type="video/mp4" />
                </video>

            </figure>
          </div>

        
          <!-- text -->
          <div class="column is-6 has-text-left reasoning-block">
            <p>
              <strong style="color:red;">When and where?</strong><br>
              <strong>Description:</strong> The boy to the left of the woman stands up and turns. 111 <br>
            </p>
        
            <p><strong>Output:</strong></p>
        
            <p class="justified-output">
              <span class="tag-label">&lt;think&gt;</span>
              To determine the event "The boy to the left of the woman stands up and turns," I need to identify the boy who is positioned to the left of the woman. In this scene, the boy wearing a striped shirt is seated next to the woman. As the video progresses, this boy stands up and begins to turn his body away from the table. The action happens around the 15-second mark.
              <span class="tag-label">&lt;/think&gt;</span>
            </p>
        
            <p><span class="tag-label">&lt;answer&gt;</span> Target ID: 3, Time range: 15.00 to 19.99 <span class="tag-label">&lt;/answer&gt;</span></p>
          </div>
        
        </div>


        <!-- case 2 -->
        <div class="columns is-vcentered">
          
          <!-- img -->
          <div class="column is-6">
            <figure class="image">
              <!-- <img src="assets/images/case1.jpg" alt="hhh" style="border-radius: 8px; box-shadow: 0 0 16px rgba(0,0,0,0.1);" /> -->
              <img src="assets/images/case2.jpg" style="border-radius: 8px; box-shadow: 0 0 16px rgba(0,0,0,0.1);" />

            </figure>
          </div>


          <!-- text -->
          <div class="column is-6 has-text-left reasoning-block">
            <p>
              <strong>Query:</strong> What is the color of the woman's jacket?<br>
              (A) black<br>
              (B) red<br>
              (C) blue<br>
              (D) orange<br>
              Answer with the option's letter from the given choices directly.
            </p>
          
            <p><strong>Output:</strong></p>
          
            <p class="justified-output">
              <span class="tag-label">&lt;think&gt;</span>
              To determine the color of the woman's jacket, I need to closely examine the area where she is located. However, the color details are not immediately clear due to the distance and lighting conditions. To improve visibility, I proceed step by step. I first identify the bounding box corresponding to the woman: 
              <span class="bbox-label">&lt;|box_start|&gt;</span>[570, 1726, 615, 1848]<span class="bbox-label">&lt;|box_end|&gt;</span>. 
              <!-- <span class="bbox-label">&lt;|box_start|&gt; [570, 1726, 615, 1848] &lt;|box_end|&gt;</span>.  -->
              I zoom in on this area to obtain a refined visual embedding 
              <span class="zoomin-label">&lt;|image_zoomin|&gt;</span>, 
              which reveals that the jacket is light blue. 
              <span class="tag-label">&lt;/think&gt;</span>
            </p>
          
            <p><span class="tag-label">&lt;answer&gt;</span> C <span class="tag-label">&lt;/answer&gt;</span></p>
          </div>
            
        </div>









        <!-- case 3 -->
        <div class="columns is-vcentered">
          
          <!-- img -->
          <div class="column is-6">
            <figure class="image">
              <!-- <img src="assets/images/case1.jpg" alt="hhh" style="border-radius: 8px; box-shadow: 0 0 16px rgba(0,0,0,0.1);" /> -->
              <img src="assets/images/case3.jpg" style="border-radius: 8px; box-shadow: 0 0 16px rgba(0,0,0,0.1);" />

            </figure>
          </div>


          <!-- text -->
          <div class="column is-6 has-text-left reasoning-block">
            <p>
              <strong>Query:</strong> What is the color of the lock?<br>
              (A) yellow<br>
              (B) silver<br>
              (C) golden<br>
              (D) red<br>
              Answer with the option's letter from the given choices directly.
            </p>
          
            <p><strong>Output:</strong></p>
          
            <p class="justified-output">
              <span class="tag-label">&lt;think&gt;</span>
              To determine the color of the lock, I need to closely examine the area where the lock is located. However, the lock is not immediately visible, making it difficult to identify its color. To improve visibility, I proceed step by step. I first attempt to locate the lock but find no clear indication due to its small size. I then focus on the red postbox, identifying two bounding boxes:
              <span class="bbox-label">&lt;|box_start|&gt;</span>[585, 700, 1305, 1821]<span class="bbox-label">&lt;|box_end|&gt;</span> and 
              <!-- <span class="bbox-label">&lt;|box_start|&gt;</span>[585, 700, 1305, 1821]<span class="bbox-label">&lt;|box_end|&gt;</span> and  -->
              <span class="bbox-label">&lt;|box_start|&gt;</span>[645, 920, 1275, 1821]<span class="bbox-label">&lt;|box_end|&gt;</span>. I analyze these areas but still cannot locate the lock. Next, I shift my attention to the area above the postbox, locating two bounding boxes:
              <!-- <span class="bbox-label">&lt;|box_start|&gt; [645, 920, 1275, 1821] &lt;|box_end|&gt;</span>. I analyze these areas but still cannot locate the lock. Next, I shift my attention to the area above the postbox, locating two bounding boxes: -->
              <span class="bbox-label">&lt;|box_start|&gt;</span>[0, 600, 1425, 1241]<span class="bbox-label">&lt;|box_end|&gt;</span> and 
              <!-- <span class="bbox-label">&lt;|box_start|&gt; [0, 600, 1425, 1241] &lt;|box_end|&gt;</span> and  -->
              <span class="bbox-label">&lt;|box_start|&gt;</span>[0, 600, 1425, 1301]<span class="bbox-label">&lt;|box_end|&gt;</span>. I zoom in on 
              <!-- <span class="bbox-label">&lt;|box_start|&gt; [0, 600, 1425, 1301] &lt;|box_end|&gt;</span>. I zoom in on  -->
              <span class="bbox-label">&lt;|box_start|&gt;</span>[0, 600, 1425, 1241]<span class="bbox-label">&lt;|box_end|&gt;</span> to obtain a refined visual embedding 
              <!-- <span class="bbox-label">&lt;|box_start|&gt; [0, 600, 1425, 1241] &lt;|box_end|&gt;</span> to obtain a refined visual embedding  -->
              <span class="zoomin-label">&lt;|image_zoomin|&gt;</span>, which reveals the presence of a mailbox. Upon analyzing this refined area, I confirm that the lock on the mailbox is silver.
              <span class="tag-label">&lt;/think&gt;</span>
            </p>
          
            <p><span class="tag-label">&lt;answer&gt;</span> B <span class="tag-label">&lt;/answer&gt;</span></p>
          </div>
        </div>





      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{zhang2025chain,
      title={Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL}, 
      author={Xintong Zhang and Zhi Gao and Bofei Zhang and Pengxiang Li and Xiaowen Zhang and Yang Liu and Tao Yuan and Yuwei Wu and Yunde Jia and Song-Chun Zhu and Qing Li},
      year={2025},
      eprint={2505.15436},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2505.15436}, 
}</code></pre>
  </div>
</section>


<!-- 
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p> Website borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> under a <a
              href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0
              International</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer> -->

<script src="static/js/contents_bar.js"></script>


</body>
</html>
